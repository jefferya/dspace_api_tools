"""
##############################################################################################
# desc: bitstream_access_control_test.py:
#     Test the HTML UI DSpace Item views to test the associated DSpace Bitstreams access control permissions
# license: CC0 1.0 Universal (CC0 1.0) Public Domain Dedication
# date: March 26, 2025
##############################################################################################
"""

import argparse
import csv
import logging

from urllib.parse import urljoin

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait

# from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException
from selenium.common.exceptions import TimeoutException


from utils import utilities as utils


def parse_args():
    """
    Parse command line arguments
    """

    parser = argparse.ArgumentParser()

    parser.add_argument(
        "--input",
        required=True,
        help="Location of a CSV generated by compare_csv with a column 'id_field'.",
    )
    parser.add_argument(
        "--output",
        required=True,
        help="Location to store CSV output.",
    )
    parser.add_argument(
        "--id_field",
        required=True,
        help="The ID field in the CSV to use as a DSpace URL.",
        default="dspace_id",
    )
    parser.add_argument(
        "--root_url",
        required=True,
        help="The root URL to the DSpace server [http://localhost:4000].",
        default="http://localhost:4000",
    )
    parser.add_argument(
        "--logging_level", required=False, help="Logging level.", default="INFO"
    )

    return parser.parse_args()


#
def get_url(root_url, item_id):
    """
    Given an ID, get the associated response page
    """
    return urljoin(root_url, f"items/{item_id}/full")


def populate_result(identifier="", bitstream_url="", access="", note=""):
    """
    Build the dict for populating the CSV output
    """
    return {
        "item_id": f"{identifier}",
        "bitstream": f"{bitstream_url}",
        "access_restriction": f"{access}",
        "note": f"{note}",
    }


#
def process(csv_reader, csv_writer, web_driver, id_field, root_url):
    """
    Process CSV rows and call DSpace delete
    """

    for row in csv_reader:
        logging.info(row[id_field])

        url = get_url(root_url, row[id_field])
        web_driver.get(url)

        logging.debug(web_driver.page_source)

        try:
            # Wait until dynamic page rendering completed
            wait = WebDriverWait(web_driver, 30)
            # Check that the Download links are present
            # element = wait.until(EC.presence_of_element_located((By.XPATH, "//ds-file-download-link")))
            elements = wait.until(
                lambda web_driver: web_driver.find_elements(
                    By.XPATH, "//ds-file-download-link[descendant::a/@href]"
                )
            )

            for element in elements:
                # logging.info("Download element rendered [%s]", element.text)
                # logging.info("Download element rendered [%s]", dir(element))
                logging.info(
                    "Download element rendered [%s]", element.get_attribute("outerHTML")
                )
                span_list = element.find_elements(By.XPATH, "a/span[@aria-label]")
                access = span_list[0].get_attribute("aria-label") if span_list else ""
                a_list = element.find_elements(By.XPATH, "a[@href]")
                bitstream_url = a_list[0].get_attribute("href") if a_list else ""

                logging.info(
                    "Bitstream id[%s] access restrictions[%s] url[%s]",
                    row[id_field],
                    access,
                    bitstream_url,
                )

                csv_writer.writerow(
                    populate_result(row[id_field], bitstream_url, access)
                )

            logging.debug(web_driver.page_source)
        except NoSuchElementException as e:
            logging.info(
                "Element not exist: id[%s] [%s]", row[id_field], e.__class__.__name__
            )
            csv_writer.writerow(
                populate_result(row[id_field], note=f"{e.__class__.__name__}")
            )
        except TimeoutException as e:
            logging.info("Timeout: id[%s] [%s]", row[id_field], e.__class__.__name__)
            csv_writer.writerow(
                populate_result(row[id_field], note=f"{e.__class__.__name__}")
            )


#
def main():
    """
    Main entry point
    """

    args = parse_args()

    utils.configure_logging(args.logging_level)

    web_options = Options()
    web_options.add_argument("--headless")
    # web_options.add_argument("--disable-gpu")
    web_options.add_argument("--window-size=1920,1080")
    web_options.add_argument("--no-sandbox")
    web_options.timeout = {"script": 30000}
    web_driver = webdriver.Chrome(options=web_options)

    output_header = populate_result().keys()

    with open(args.input, "r", encoding="utf-8", newline="") as input_file:
        csv_reader = csv.DictReader(input_file)
        with open(args.output, "w", encoding="utf-8", newline="") as output_file:
            csv_writer = csv.DictWriter(output_file, fieldnames=output_header)
            csv_writer.writeheader()
            process(csv_reader, csv_writer, web_driver, args.id_field, args.root_url)

    web_driver.quit()


#
if __name__ == "__main__":
    main()
